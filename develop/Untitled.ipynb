{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c580716-1bd9-4771-a7bd-6d4f0f8f8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
    "import pytorch_lightning as pl\n",
    "from pprint import pprint \n",
    "\n",
    "# 日本語学習済みモデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae8fb4f-6a8e-4ce5-afeb-8b118f9691f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-5\n",
    "class NER_tokenizer(BertJapaneseTokenizer):\n",
    "       \n",
    "    def encode_plus_tagged(self, text: str, entities: list, max_length: int) -> dict:\n",
    "        \"\"\"\n",
    "        文章とそれに含まれる固有表現が与えられた時に、\n",
    "        符号化とラベル列の作成を行う。正解ラベルの作成を行う\n",
    "        \"\"\"\n",
    "        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
    "        entities = sorted(entities, key=lambda x: x['span'][0])\n",
    "        splitted = [] # 分割後の文字列を追加していく\n",
    "        position = 0\n",
    "        for entity in entities:\n",
    "            start = entity['span'][0]\n",
    "            end = entity['span'][1]\n",
    "            label = entity['type_id']\n",
    "            # 固有表現ではないものには0のラベルを付与\n",
    "            splitted.append({'text':text[position:start], 'label':0}) \n",
    "            # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n",
    "            splitted.append({'text':text[start:end], 'label':label}) \n",
    "            position = end\n",
    "        splitted.append({'text': text[position:], 'label':0})\n",
    "        splitted = [ s for s in splitted if s['text'] ] # 長さ0の文字列は除く\n",
    "        # print(splitted)\n",
    "\n",
    "        # 分割されたそれぞれの文字列をトークン化し、ラベルをつける。\n",
    "        tokens = [] # トークンを追加していく\n",
    "        labels = [] # トークンのラベルを追加していく\n",
    "        for text_splitted in splitted:\n",
    "            text = text_splitted['text']\n",
    "            label = text_splitted['label']\n",
    "            tokens_splitted = tokenizer.tokenize(text) # 形態素解析\n",
    "            labels_splitted = [label] * len(tokens_splitted)\n",
    "            tokens.extend(tokens_splitted) # [\"word1\", \"word2\", \"word3\"]\n",
    "            labels.extend(labels_splitted) # [0, 0, 1, 1, 0, 0, 0]\n",
    "\n",
    "        # 符号化を行いBERTに入力できる形式にする。\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        encoding = tokenizer.prepare_for_model(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True\n",
    "        ) # input_idsをencodingに変換\n",
    "        # 特殊トークン[CLS]、[SEP]のラベルを0にする。\n",
    "        labels = [0] + labels[:max_length-2] + [0] \n",
    "        # 特殊トークン[PAD]のラベルを0にする。\n",
    "        labels = labels + [0]*( max_length - len(labels) ) \n",
    "        encoding['labels'] = labels\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(\n",
    "        self, text: str, max_length=None, return_tensors=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
    "        \"\"\"\n",
    "        # 文章のトークン化を行い、\n",
    "        # それぞれのトークンと文章中の文字列を対応づける。\n",
    "        tokens = [] # トークンを追加していく。\n",
    "        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n",
    "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
    "        for word in words:\n",
    "            # 単語をサブワードに分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
    "            tokens.extend(tokens_word) # トークン処理後の生データを入れる\n",
    "            if tokens_word[0] == '[UNK]': # トークン処理後にクレンジングをして入れる\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                tokens_original.extend([\n",
    "                    token.replace('##','') for token in tokens_word\n",
    "                ])\n",
    "\n",
    "        # print(tokens)\n",
    "        # print(tokens_original)\n",
    "\n",
    "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
    "        position = 0\n",
    "        spans = [] # トークンの位置を追加していく。\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                if token != text[position:position+l]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position+l]) # [[0, 2], [3, 4] ....]\n",
    "                    position += l\n",
    "                    break\n",
    "\n",
    "        # 符号化を行いBERTに入力できる形式にする。\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens) \n",
    "        encoding = tokenizer.prepare_for_model(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            padding='max_length' if max_length else False, \n",
    "            truncation=True if max_length else False\n",
    "        )\n",
    "        sequence_length = len(encoding['input_ids'])\n",
    "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
    "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
    "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
    "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
    "\n",
    "        # 必要に応じてtorch.Tensorにする。\n",
    "        if return_tensors == 'pt':\n",
    "            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "    def convert_bert_output_to_entities(self, text: str, labels: list, spans: list) -> list:\n",
    "        \"\"\"\n",
    "        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
    "        entitiesの作成。\n",
    "        \"\"\"\n",
    "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
    "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
    "        spans = [span for span in spans if span[0] != -1]\n",
    "\n",
    "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
    "        entities = []\n",
    "        for label, group \\\n",
    "            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
    "            \n",
    "            group = list(group)\n",
    "            # print(group)\n",
    "            start = spans[group[0][0]][0]\n",
    "            end = spans[group[-1][0]][1]\n",
    "\n",
    "            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n",
    "                entity = {\n",
    "                    \"name\": text[start:end],\n",
    "                    \"span\": [start, end],\n",
    "                    \"type_id\": label\n",
    "                }\n",
    "                entities.append(entity)\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "845f9ccc-0a6e-486e-9df6-8447889589e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 10271, 28486, 5, 546, 10780, 2464, 13, 5, 1878, 2682, 9, 10750, 308, 10, 8, 3, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], 'labels': [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "text = '昨日のみらい事務所との打ち合わせは順調だった。'\n",
    "entities = [\n",
    "    {'name': 'みらい事務所', 'span': [3,9], 'type_id': 1}\n",
    "]\n",
    "\n",
    "encoding = tokenizer.encode_plus_tagged(\n",
    "    text, entities, max_length=20\n",
    ")\n",
    "\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f319a7-a0d8-43cb-a020-326c99c03376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ner-wikipedia-dataset'...\n",
      "remote: Enumerating objects: 32, done.\u001b[K\n",
      "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
      "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
      "remote: Total 32 (delta 9), reused 11 (delta 1), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (32/32), 663.49 KiB | 521.00 KiB/s, done.\n",
      "Note: switching to 'f7ed83626d90e5a79f1af99775e4b8c6cba15295'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "pip install transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.2.7\n",
    "\n",
    "git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5b66a3-8483-4380-9f0c-280a8bbbe291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadWikipedia:\n",
    "    def __init__(self, model_name, filename):\n",
    "        \"\"\"\n",
    "        wikipediaのデータセット読み込み専用のクラスです。\n",
    "        独自のデータセットより学習する場合は、dateloader(バッチ分割)した訓練、検証、テストデータをそれぞれ返してください\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dataset = []\n",
    "        self.tokenizer = NER_tokenizer.from_pretrained(model_name)\n",
    "        self.type_id_dict = {\n",
    "            \"人名\": 1,\n",
    "            \"法人名\": 2,\n",
    "            \"政治的組織名\": 3,\n",
    "            \"その他の組織名\": 4,\n",
    "            \"地名\": 5,\n",
    "            \"施設名\": 6,\n",
    "            \"製品名\": 7,\n",
    "            \"イベント名\": 8\n",
    "        }\n",
    "        self.filename = filename \n",
    "    def _load_dataset(self):\n",
    "        dataset = json.load(open(self.filename, \"r\"))\n",
    "        for sample in dataset:\n",
    "            del sample[\"curid\"]\n",
    "            sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
    "            for e in sample[\"entities\"]:\n",
    "                e['type_id'] = self.type_id_dict[e['type']]\n",
    "                del e['type']\n",
    "        self.dataset = dataset\n",
    "    def create_dataset(self, max_length: int, batch_size: int, cuda: bool):\n",
    "        self._load_dataset()\n",
    "        random.shuffle(self.dataset)\n",
    "        n = len(self.dataset)\n",
    "        n_train = int(n*.6)\n",
    "        n_val = int(n*.2)\n",
    "        dataset_train = self.dataset[:n_train]\n",
    "        dataset_val = self.dataset[n_train:n_train+n_val]\n",
    "        dataset_test = self.dataset[n_train+n_val:]\n",
    "        # bertに入力できるデータに変換する\n",
    "        dataloader_train = self._create_dataloader(dataset_train, max_length, cuda)\n",
    "        dataloader_val = self._create_dataloader(dataset_val, max_length, cuda)\n",
    "#         dataloader_test = self._create_dataloader(dataset_test, max_length, cuda)\n",
    "        # バッチ単位で分割する\n",
    "        dataloader_train = DataLoader(\n",
    "            dataloader_train, batch_size=batch_size, shuffle=True\n",
    "        ) # (batch_size, max_length, 1)\n",
    "        dataloader_val = DataLoader(\n",
    "            dataloader_val, batch_size=256\n",
    "        )\n",
    "#         dataloader_test = DataLoader(\n",
    "#             dataloader_test, batch_size=256\n",
    "#         )\n",
    "        \n",
    "        # デバック用\n",
    "#         for t in dataloader_train:\n",
    "#             print(t[\"input_ids\"].size())\n",
    "#             break\n",
    "        return dataloader_train, dataloader_val, dataset_test\n",
    "        \n",
    "        \n",
    "    def _create_dataloader(self, dataset: list, max_length: int, cuda: bool):\n",
    "        dataloader = []\n",
    "        for data in dataset:\n",
    "            text = data[\"text\"]\n",
    "            entities = data[\"entities\"]\n",
    "            # textのトークン化、正解ラベルの付与\n",
    "            encoding = self.tokenizer.encode_plus_tagged(\n",
    "                text, entities, max_length=max_length\n",
    "            )\n",
    "            encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "            if cuda:\n",
    "                encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "            dataloader.append(encoding)\n",
    "        return dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd5cfca-2279-4ece-a051-3469f65e932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightningのモデル\n",
    "class BertForTokenClassification_pl(pl.LightningModule):\n",
    "        \n",
    "    def __init__(self, model_name: str, num_labels: int, lr: float):\n",
    "        \"\"\"\n",
    "        `BertForTokenClassification`を使った固有表現抽出タスクを実行します。\n",
    "        モデルの層では、\n",
    "          (文章数, 単語数, 1) 入力層\n",
    "            -> (文章数, 単語数, 768) BertModelによる単語埋め込み処理\n",
    "              -> (文章数, 単語数, )\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # (32, 128, 1) -> (32, 128, 768) -> (32, 128, 4)\n",
    "        self.bert_tc = BertForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        print(self.bert_tc.config)\n",
    "        print(\"~\"*100)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_tc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_tc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2f547bb-a6ff-42a4-9109-93a54427cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint \n",
    "class MyBertForTokenClassification_from_wikipedia:\n",
    "    def __init__(self,\n",
    "                 model_name=\"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
    "                 filename=\"ner-wikipedia-dataset/ner.json\"):\n",
    "        self.model_name = model_name \n",
    "        self.filename = filename \n",
    "        self.wiki = LoadWikipedia(model_name, filename)\n",
    "        self.type_id_dict = self.wiki.type_id_dict\n",
    "        self.model = None\n",
    "        self.best_model_path = \"\"\n",
    "        self.max_length = 0\n",
    "        self.i = 0\n",
    "        self.tokenizer = None\n",
    "    def _trainer(self, cuda: bool):\n",
    "        \"\"\"\n",
    "        pytorch-lightningの学習時の`Trainer`インスタンスを生成します。\n",
    "        学習時にcheckpointを記録し、検証データのlossを指標とします。\n",
    "        \"\"\"\n",
    "        checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_top_k=1,\n",
    "            save_weights_only=True,\n",
    "            dirpath='model/'\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=1,\n",
    "            callbacks=[checkpoint],\n",
    "            gpus:1 if cuda else 0\n",
    "        )\n",
    "        return trainer, checkpoint\n",
    "    def _eval(self, entities_pred_list, entities_list):\n",
    "        \"\"\"\n",
    "        テストデータを使ってモデルの評価関数を作成します。\n",
    "        評価条件としては、１文章当たりのentitiesが予測したentitiesの数分だけ正解とします。\n",
    "        例: \n",
    "            + correct [1, 1, 0]\n",
    "            + predict [1, 0]\n",
    "            この場合はcorrect == predict が最初のみ正解のため,\n",
    "            　　正解率は1/3(33.33%)になります。\n",
    "                適合率は1/2(50.0%) -- 予測したものの中で正解だったデータ数\n",
    "                再現率は1/3(33.33%) -- 正解ラベルのうち予測が正解だったデータ数\n",
    "            \n",
    "        \"\"\"\n",
    "        num_all, num_pred, num_correct = 0, 0, 0\n",
    "        for entities, pred in zip(entities_list, entities_pred_list):\n",
    "            get_span_id = lambda e: (e[\"span\"][0], e[\"span\"][1], e[\"type_id\"])\n",
    "            set_entities = set(get_span_id(e) for e in entities)\n",
    "            set_entities_pred = set(get_span_id(e) for e in pred)\n",
    "            \n",
    "            num_all += len(entities) # 全entitiesの数\n",
    "            num_pred += len(pred) # 予測数の累計\n",
    "            num_correct += len(set_entities & set_entities_pred) # 予測が正しかった数の累計\n",
    "            \n",
    "        precision = num_correct/num_pred \n",
    "        recall = num_correct/num_all\n",
    "        f_value = 2*precision*recall/(precision+recall)\n",
    "        \n",
    "        result = {\n",
    "            'num_entities': num_all,\n",
    "            'num_predictions': num_pred,\n",
    "            'num_correct': num_correct,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f_value': f_value\n",
    "        }\n",
    "        print(\"テストデータの評価指標: \\n\")\n",
    "        pprint(result)\n",
    "    def _test(self, test, cuda):\n",
    "        entities_list , entities_pred_list = [], []\n",
    "        for i, data in enumerate(tqdm(test)):\n",
    "            text = data[\"text\"]\n",
    "            entities_predict = self._predict_(text, cuda)\n",
    "            entities_pred_list.append(entities_predict) # 予測\n",
    "            entities_list.append(data[\"entities\"]) # 正解\n",
    "        self._eval(entities_pred_list, entities_list)\n",
    "            \n",
    "    def train(self, max_length=128, batch_size=32, cuda=False, lr=0.01):\n",
    "        \"\"\"\n",
    "        BertForTokenClassificationのファインチューニング学習をします。\n",
    "        wikipediaのデータセットを使用します。\n",
    "        独自のデータセットで学習を行い場合には、訓練と検証データは`DataLoader`型で、\n",
    "        テストデータは{\"text\", \"entities\"}をキーとするデータのリスト型で返してください。\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        # dataloaderの取得\n",
    "        train, val, test = self.wiki.create_dataset(max_length, batch_size, cuda)\n",
    "        num_labels = len(self.type_id_dict)+1 # 出力層のclass数\n",
    "        # モデルの読み込み\n",
    "        self.model = BertForTokenClassification_pl(self.model_name, \n",
    "                                                   num_labels=num_labels,\n",
    "                                                   lr=lr)\n",
    "        if cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        # 学習\n",
    "        trainer, checkpoint = self._trainer(cuda)\n",
    "        trainer.fit(self.model, train, val)\n",
    "        # 最もlossの少ないモデルの保存\n",
    "        self.best_model_path = checkpoint.best_model_path \n",
    "        self._test(test, cuda)\n",
    "        print(f\"best_model_path: {self.best_model_path}\")\n",
    "        self.model = None \n",
    "    def _load_model_(self, cuda: bool):\n",
    "        if self.best_model_path != \"\":\n",
    "            model = BertForTokenClassification_pl.load_from_checkpoint(\n",
    "                self.best_model_path\n",
    "            )\n",
    "            self.model = model.bert_tc # bertのモデルとcheckpointの適合\n",
    "            if cuda:\n",
    "                self.model = self.model.cuda()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "    def _predict_(self, text: str, cuda=False):\n",
    "        \"\"\"\n",
    "        学習時のテストデータの推論を行います。\n",
    "        \"\"\"\n",
    "        # wikiクラスにあるtokenizerをそのまま使用する\n",
    "        self.i += 1 \n",
    "        if self.i == 1:\n",
    "            self._load_model_(cuda)\n",
    "            self.tokenizer = self.wiki.tokenizer\n",
    "        encoding, spans = self.tokenizer.encode_plus_untagged(\n",
    "            text, return_tensors=\"pt\"\n",
    "        )\n",
    "        if cuda:\n",
    "            encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "        # 推論\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**encoding)\n",
    "            scores = output.logits # トークンごとの属するクラスの確率\n",
    "            predict = scores[0].argmax(-1).cpu().numpy().tolist()\n",
    "        # 文章から予測クラスに従って、固有表現のリストを取得する\n",
    "        entities = self.tokenizer.convert_bert_output_to_entities(\n",
    "            text, predict, spans\n",
    "        )\n",
    "        \n",
    "    def predict(self, text: str, filepath=\"\", cuda=False):\n",
    "        \"\"\"\n",
    "        wikipediaで学習したモデルを使って、文章から固有表現の抽出を行います。\n",
    "        `text`に文章を入力して実行してください。\n",
    "        gpuも指定できますが、事前学習済モデルのため多くの場合で必要ありません。\n",
    "        \"\"\"\n",
    "        self._load_model(filepath, cuda)\n",
    "        self.tokenizer = self.wiki.tokenizer \n",
    "        encoding, span = self.tokenizer.encode_plus_untagged(\n",
    "            text, return_tensors=\"pt\"\n",
    "        )\n",
    "        if cuda:\n",
    "            encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**encoding)\n",
    "            scores = output.logits \n",
    "            predict = scores[0].argmax(-1).cpu().numpy().tolist()\n",
    "        entities = self.tokenizer.convert_bert_output_to_entities(\n",
    "            text, predict, spans\n",
    "        )\n",
    "        category = self.wiki.type_id_dict\n",
    "        for entity in entities:\n",
    "            entity_list = entity[0]\n",
    "            name = entity_list[\"name\"]\n",
    "#             span = entity_list[\"span\"]\n",
    "            type_id = entity_list[\"type_id\"]\n",
    "            cate = category[type_id]\n",
    "            print(\"--\")\n",
    "            print(f\"name: {name}\")\n",
    "            print(f\"category: {cate}\")\n",
    "        self.model = None \n",
    "    def _load_model(self, filepath, cuda):\n",
    "        if filepath != \"\":\n",
    "            model = BertForTokenClassification_pl.load_from_checkpoint(\n",
    "                filepath\n",
    "            )\n",
    "            self.model = model \n",
    "            if cuda:\n",
    "                self.model = self.model.cuda()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4875e57-8be0-45be-be8a-e3d8edcea7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyBertForTokenClassification_from_wikipedia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba58eefa-b45a-4677-97c7-a60f5f4a9a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ff522d5814417ca962ae2e46bf3783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
      "  \"transformers_version\": \"4.5.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name    | Type                       | Params\n",
      "-------------------------------------------------------\n",
      "0 | bert_tc | BertForTokenClassification | 110 M \n",
      "-------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.135   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb81bb23c4e496d8b81a6cd383f7ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_474/1676823408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_474/2504966218.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, max_length, batch_size, cuda, lr)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# 学習\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# 最もlossの少ないモデルの保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_or_test_or_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_bar_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;31m# set stage for logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_sanity_val_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_sanity_check_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(self, max_batches, on_epoch)\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluation_step_and_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# capture any logged information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_474/3421696131.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1677\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1679\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1680\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         )\n\u001b[0;32m--> 971\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    972\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    566\u001b[0m                 )\n\u001b[1;32m    567\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    569\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     ):\n\u001b[0;32m--> 387\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-lab/bert/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aabc879-9eb9-4555-b388-cd325492caca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 2, 1, 1, 1, 0, 3, 3, 1, 0, 2, 0, 2, 1, 3, 2, 2, 2, 1, 3, 2, 3, 0,\n",
       "         0, 3, 2, 2, 3, 0, 1, 0, 2, 3, 3, 3, 1, 2, 1, 2, 0, 2, 3, 2, 3, 2, 1, 0,\n",
       "         2, 1, 3, 2, 1, 0, 1, 0, 1, 2, 0, 1, 3, 2, 1, 3, 1, 0, 1, 3, 1, 2, 1, 1,\n",
       "         2, 0, 0, 0, 3, 0, 1, 1, 2, 1, 2, 0, 0, 0, 1, 3, 1, 2, 3, 3, 3, 0, 2, 2,\n",
       "         2, 0, 3, 0, 0, 2, 0, 2, 2, 2, 3, 1, 2, 2, 2, 2, 1, 2, 3, 0, 1, 0, 2, 2],\n",
       "        [1, 1, 2, 0, 2, 3, 0, 1, 0, 0, 0, 1, 1, 0, 3, 0, 2, 3, 2, 3, 1, 2, 2, 0,\n",
       "         3, 0, 2, 1, 0, 2, 2, 2, 1, 2, 3, 0, 2, 1, 3, 3, 3, 1, 2, 1, 1, 3, 1, 3,\n",
       "         1, 3, 3, 3, 2, 2, 1, 0, 1, 3, 3, 2, 2, 1, 2, 1, 2, 2, 1, 0, 2, 1, 2, 3,\n",
       "         3, 0, 3, 2, 2, 0, 0, 3, 0, 2, 1, 2, 3, 2, 1, 3, 0, 2, 0, 3, 3, 0, 3, 1,\n",
       "         2, 1, 1, 0, 2, 3, 3, 2, 3, 3, 0, 0, 0, 0, 3, 3, 3, 1, 0, 2, 3, 2, 1, 3],\n",
       "        [2, 0, 1, 1, 2, 3, 1, 0, 2, 2, 1, 0, 0, 1, 2, 1, 1, 2, 1, 2, 2, 0, 3, 1,\n",
       "         2, 2, 2, 0, 2, 2, 2, 3, 2, 3, 2, 0, 2, 0, 1, 1, 3, 1, 1, 2, 2, 2, 3, 1,\n",
       "         0, 2, 3, 2, 2, 2, 3, 2, 0, 3, 0, 0, 1, 1, 1, 2, 0, 0, 2, 0, 2, 0, 3, 2,\n",
       "         0, 2, 2, 1, 3, 0, 1, 0, 2, 1, 2, 2, 1, 0, 3, 2, 2, 1, 2, 0, 1, 0, 0, 1,\n",
       "         0, 0, 1, 2, 1, 2, 2, 1, 0, 3, 1, 2, 2, 0, 0, 2, 1, 2, 1, 0, 2, 0, 3, 2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((3, 120, 4))\n",
    "\n",
    "a.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "939adf0d-9107-466c-9156-9adb0ffd08be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.3070e-01, 2.2404e-01, 5.4846e-03, 3.5957e-01, 8.9594e-01,\n",
       "          5.5020e-01, 9.3995e-01, 5.4217e-01, 6.6849e-02, 4.6496e-01,\n",
       "          8.8257e-01, 4.3787e-01, 4.2099e-01, 4.3236e-01, 6.0123e-01,\n",
       "          6.6666e-01, 4.9848e-02, 7.3408e-01, 4.3768e-02, 3.3860e-01,\n",
       "          2.5364e-01, 6.0454e-01, 9.2282e-01, 2.1235e-01, 3.3379e-01,\n",
       "          2.7169e-01, 5.5289e-03, 8.3089e-01, 7.3584e-01, 2.8690e-01,\n",
       "          9.6513e-01, 8.0132e-02, 3.9400e-01, 1.7237e-01, 4.2546e-01,\n",
       "          9.2700e-01, 2.6023e-01, 6.6298e-01, 9.7798e-01, 5.6739e-01,\n",
       "          3.9487e-01, 1.4848e-01, 5.2263e-01, 1.6742e-01, 4.4138e-01,\n",
       "          9.9504e-01, 8.0433e-01, 7.3362e-01, 5.1287e-01, 8.3422e-01,\n",
       "          6.4682e-01, 7.4494e-01, 1.0466e-01, 9.9818e-01, 6.5719e-01,\n",
       "          9.5469e-01, 8.3655e-01, 8.6032e-02, 5.0289e-01, 3.5904e-01,\n",
       "          5.3258e-01, 7.8781e-01, 8.6027e-01, 3.1560e-01, 3.0962e-01,\n",
       "          8.5857e-02, 6.9165e-01, 2.2384e-01, 8.0850e-01, 3.5640e-01,\n",
       "          8.3193e-01, 2.3936e-01, 4.1091e-01, 6.6229e-02, 9.2589e-01,\n",
       "          5.4688e-01, 8.1133e-01, 2.8050e-01, 4.2358e-01, 9.5539e-01,\n",
       "          9.8189e-01, 9.0968e-02, 2.3010e-01, 7.0934e-01, 4.2991e-01,\n",
       "          1.6031e-01, 6.5616e-02, 4.6899e-01, 9.8603e-01, 3.4562e-01,\n",
       "          5.8787e-01, 7.6851e-01, 8.9402e-01, 5.1708e-01, 7.8204e-01,\n",
       "          2.9208e-01, 4.4167e-01, 9.0477e-01, 5.7059e-01, 5.3539e-02,\n",
       "          7.0770e-01, 1.2833e-01, 5.2511e-01, 3.6891e-01, 7.9658e-01,\n",
       "          8.7025e-01, 8.7367e-01, 5.4015e-01, 7.9357e-01, 5.7243e-01,\n",
       "          8.2739e-01, 1.2901e-01, 1.7085e-01, 4.2027e-01, 3.1925e-01,\n",
       "          9.5122e-01, 8.3472e-03, 7.3350e-01, 2.5399e-01, 7.4232e-01,\n",
       "          8.2929e-01, 6.8866e-01, 4.1766e-01, 1.2361e-01, 7.4616e-01,\n",
       "          5.1343e-01, 6.0015e-02, 1.9406e-01, 2.8056e-04],\n",
       "         [1.6433e-01, 4.7861e-01, 6.7317e-01, 9.0422e-01, 1.6117e-01,\n",
       "          6.4682e-02, 4.6523e-01, 9.5190e-01, 1.9781e-01, 6.6252e-01,\n",
       "          2.7056e-01, 7.6532e-01, 8.7846e-01, 1.7761e-01, 7.1386e-01,\n",
       "          6.9593e-01, 3.8548e-01, 7.0432e-01, 1.7275e-02, 3.4867e-01,\n",
       "          8.7186e-01, 4.3246e-01, 5.7307e-01, 1.4258e-01, 5.4397e-01,\n",
       "          3.3877e-01, 1.6070e-01, 4.1795e-01, 8.8882e-01, 7.2910e-02,\n",
       "          3.8867e-01, 9.3459e-01, 3.3646e-03, 8.0558e-01, 3.7426e-02,\n",
       "          8.8481e-01, 7.5923e-01, 2.3030e-01, 6.9772e-01, 5.6910e-01,\n",
       "          6.8242e-01, 8.7659e-01, 8.5645e-01, 9.2360e-01, 4.9729e-01,\n",
       "          5.1188e-02, 5.4642e-01, 4.6763e-01, 4.6076e-01, 4.5768e-01,\n",
       "          4.8512e-01, 7.4964e-02, 5.8385e-01, 1.5074e-01, 2.6047e-01,\n",
       "          3.2393e-01, 9.0381e-01, 4.8492e-01, 6.5725e-01, 8.4310e-01,\n",
       "          1.2981e-01, 1.4254e-01, 4.4521e-01, 6.5987e-01, 3.2709e-01,\n",
       "          9.4414e-01, 5.2478e-01, 3.4809e-01, 5.7455e-01, 3.0467e-02,\n",
       "          2.7734e-01, 1.7361e-02, 9.5469e-01, 5.6647e-01, 8.9640e-01,\n",
       "          1.1953e-01, 5.8323e-01, 3.6040e-01, 4.3881e-01, 6.3230e-01,\n",
       "          8.9931e-01, 1.4310e-01, 5.5061e-01, 3.0488e-02, 9.8715e-01,\n",
       "          9.0769e-01, 6.2538e-01, 8.1824e-01, 5.6508e-01, 2.7347e-01,\n",
       "          7.6102e-01, 3.8591e-02, 6.5428e-01, 8.3905e-01, 7.2298e-01,\n",
       "          3.2074e-01, 9.1460e-01, 3.7463e-01, 4.5202e-01, 5.5640e-01,\n",
       "          4.6503e-01, 6.1542e-01, 4.4250e-01, 3.6785e-01, 2.7930e-01,\n",
       "          8.2443e-01, 5.7045e-01, 6.7881e-01, 3.5724e-01, 1.7208e-01,\n",
       "          3.3612e-01, 9.9644e-02, 1.2691e-02, 3.1362e-01, 5.6619e-01,\n",
       "          9.2367e-01, 9.9957e-01, 6.2237e-01, 8.9856e-01, 4.6412e-01,\n",
       "          4.0406e-01, 1.6407e-01, 9.4817e-01, 5.8074e-01, 3.0667e-01,\n",
       "          4.0440e-01, 4.2085e-01, 6.5993e-01, 6.8763e-01],\n",
       "         [7.3362e-01, 4.5440e-01, 2.8525e-01, 3.8544e-01, 7.0747e-01,\n",
       "          1.9333e-01, 3.5755e-01, 8.9743e-01, 8.4653e-01, 1.1324e-01,\n",
       "          1.3217e-01, 6.4784e-01, 3.0330e-01, 2.1735e-01, 5.4457e-01,\n",
       "          3.4048e-01, 5.2287e-01, 4.3950e-01, 1.9612e-01, 4.3086e-01,\n",
       "          7.8653e-01, 3.7229e-01, 1.5603e-01, 3.6767e-01, 7.6024e-01,\n",
       "          3.4845e-01, 8.1807e-01, 5.5337e-01, 2.3708e-01, 7.1980e-01,\n",
       "          5.1287e-01, 4.0193e-01, 7.2267e-01, 9.7056e-01, 8.0930e-02,\n",
       "          4.6764e-01, 7.4110e-02, 7.4076e-01, 6.6794e-01, 1.9984e-01,\n",
       "          5.7038e-01, 8.7153e-01, 7.1708e-01, 4.0328e-01, 8.7762e-01,\n",
       "          1.4557e-01, 8.3681e-01, 7.0925e-02, 9.4123e-01, 6.5886e-01,\n",
       "          9.7214e-01, 9.9482e-01, 3.1995e-01, 6.9232e-01, 6.3657e-01,\n",
       "          6.3485e-01, 2.3306e-01, 9.3915e-01, 3.3029e-01, 3.2316e-01,\n",
       "          3.7977e-01, 3.3646e-01, 9.8391e-01, 7.8179e-01, 5.5217e-02,\n",
       "          5.1972e-01, 2.2272e-01, 3.5718e-01, 3.5883e-01, 2.6279e-01,\n",
       "          9.0346e-01, 3.9729e-01, 4.9037e-01, 6.6105e-01, 1.2742e-01,\n",
       "          9.2224e-01, 2.8400e-01, 9.8272e-02, 4.3794e-01, 7.2640e-01,\n",
       "          2.8859e-01, 3.6821e-01, 5.3217e-01, 7.0202e-01, 3.1098e-02,\n",
       "          1.1313e-01, 7.9191e-01, 9.0050e-01, 2.4486e-01, 4.4859e-01,\n",
       "          2.2198e-01, 8.9301e-01, 4.8072e-01, 7.5772e-01, 3.5965e-01,\n",
       "          9.3843e-01, 6.7856e-01, 4.6552e-01, 9.0276e-01, 8.1295e-01,\n",
       "          9.6266e-01, 5.8485e-02, 2.9239e-01, 1.2781e-01, 7.3071e-01,\n",
       "          6.4541e-01, 9.1377e-01, 3.0979e-01, 8.9384e-01, 4.7312e-01,\n",
       "          8.7151e-02, 3.7597e-01, 2.7146e-01, 8.9573e-01, 6.5158e-01,\n",
       "          8.8498e-02, 3.8074e-01, 4.0301e-01, 9.1219e-01, 5.1432e-01,\n",
       "          9.6078e-01, 1.0987e-01, 2.8416e-01, 1.6616e-01, 6.7474e-02,\n",
       "          5.0282e-01, 4.2435e-01, 1.9535e-01, 3.8732e-01]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0355303-6e70-4c02-8429-bdbc4d3957a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
